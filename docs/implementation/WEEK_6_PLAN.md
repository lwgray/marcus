## **Week 6: Validation, Documentation & MVP Release**

**Goal**: Final validations, comprehensive documentation, deployment enhancements, and MVP release preparation.

### **Monday: Core Validations (Issues #118-125)**

**Goal**: Implement critical validations to prevent system errors and improve reliability.

**Background**: Issues #118-125 identified missing validations that can cause runtime errors or data inconsistencies:
- #118: Validate task dependencies exist before assignment
- #119: Validate agent exists before task assignment
- #120: Validate project exists before creating features
- #121: Validate feature exists before creating tasks
- #122: Validate task status transitions are valid
- #123: Validate estimated hours are positive
- #124: Validate priority values are valid
- #125: Validate agent skills match task requirements (warning only)

#### **Step 1: Create Validation Framework**

Create `src/core/validators.py`:

```python
"""
Core validation framework for Marcus.

Provides reusable validators for tasks, agents, projects, and features
to prevent runtime errors and data inconsistencies.
"""

from typing import Any, Dict, List, Optional

from src.core.error_framework import (
    ErrorContext,
    MarcusBaseError,
    ValidationError,
)
from src.core.models import Priority, TaskStatus


class TaskValidator:
    """
    Validates task operations.

    Examples
    --------
    >>> validator = TaskValidator(state)
    >>> validator.validate_task_creation(task_data)
    >>> validator.validate_task_assignment(task_id, agent_id)
    """

    def __init__(self, state: Any):
        """
        Initialize task validator.

        Parameters
        ----------
        state : Any
            State manager instance.
        """
        self.state = state

    def validate_task_dependencies(
        self,
        task_id: str,
        dependencies: List[str],
    ) -> None:
        """
        Validate that all task dependencies exist.

        Parameters
        ----------
        task_id : str
            The task ID.
        dependencies : List[str]
            List of dependency task IDs.

        Raises
        ------
        ValidationError
            If any dependency doesn't exist.

        Examples
        --------
        >>> validator.validate_task_dependencies("T-1", ["T-2", "T-3"])
        """
        missing = []
        for dep_id in dependencies:
            if dep_id not in self.state.tasks:
                missing.append(dep_id)

        if missing:
            raise ValidationError(
                f"Task {task_id} has invalid dependencies: {missing}",
                context=ErrorContext(
                    operation="validate_dependencies",
                    task_id=task_id,
                    additional_info={"missing_dependencies": missing},
                ),
            )

    def validate_agent_exists(
        self,
        agent_id: str,
        operation: str = "task_assignment",
    ) -> None:
        """
        Validate that agent exists.

        Parameters
        ----------
        agent_id : str
            The agent ID.
        operation : str
            The operation being performed.

        Raises
        ------
        ValidationError
            If agent doesn't exist.

        Examples
        --------
        >>> validator.validate_agent_exists("agent-1", "task_assignment")
        """
        if agent_id not in self.state.agents:
            raise ValidationError(
                f"Agent {agent_id} does not exist",
                context=ErrorContext(
                    operation=operation,
                    agent_id=agent_id,
                ),
            )

    def validate_status_transition(
        self,
        task_id: str,
        current_status: TaskStatus,
        new_status: TaskStatus,
    ) -> None:
        """
        Validate task status transition is valid.

        Parameters
        ----------
        task_id : str
            The task ID.
        current_status : TaskStatus
            Current status.
        new_status : TaskStatus
            New status.

        Raises
        ------
        ValidationError
            If transition is invalid.

        Examples
        --------
        >>> validator.validate_status_transition("T-1", TaskStatus.TODO, TaskStatus.IN_PROGRESS)
        """
        # Define valid transitions
        valid_transitions = {
            TaskStatus.TODO: [TaskStatus.IN_PROGRESS, TaskStatus.BLOCKED],
            TaskStatus.IN_PROGRESS: [
                TaskStatus.COMPLETED,
                TaskStatus.BLOCKED,
                TaskStatus.TODO,
            ],
            TaskStatus.BLOCKED: [TaskStatus.TODO, TaskStatus.IN_PROGRESS],
            TaskStatus.COMPLETED: [],  # No transitions from completed
        }

        allowed = valid_transitions.get(current_status, [])

        if new_status not in allowed:
            raise ValidationError(
                f"Invalid status transition for task {task_id}: "
                f"{current_status.value} -> {new_status.value}",
                context=ErrorContext(
                    operation="status_transition",
                    task_id=task_id,
                    additional_info={
                        "current_status": current_status.value,
                        "new_status": new_status.value,
                        "allowed_transitions": [s.value for s in allowed],
                    },
                ),
            )

    def validate_estimated_hours(
        self,
        task_id: str,
        estimated_hours: float,
    ) -> None:
        """
        Validate estimated hours are positive.

        Parameters
        ----------
        task_id : str
            The task ID.
        estimated_hours : float
            Estimated hours.

        Raises
        ------
        ValidationError
            If estimated hours are not positive.

        Examples
        --------
        >>> validator.validate_estimated_hours("T-1", 4.0)
        """
        if estimated_hours <= 0:
            raise ValidationError(
                f"Task {task_id} has invalid estimated hours: {estimated_hours}",
                context=ErrorContext(
                    operation="validate_estimated_hours",
                    task_id=task_id,
                    additional_info={"estimated_hours": estimated_hours},
                ),
            )

    def validate_priority(
        self,
        task_id: str,
        priority: Any,
    ) -> Priority:
        """
        Validate priority value is valid.

        Parameters
        ----------
        task_id : str
            The task ID.
        priority : Any
            Priority value (string or Priority enum).

        Returns
        -------
        Priority
            Validated priority enum.

        Raises
        ------
        ValidationError
            If priority is invalid.

        Examples
        --------
        >>> priority = validator.validate_priority("T-1", "high")
        >>> print(priority)
        Priority.HIGH
        """
        if isinstance(priority, Priority):
            return priority

        # Convert string to enum
        try:
            return Priority(priority.lower())
        except (ValueError, AttributeError):
            raise ValidationError(
                f"Task {task_id} has invalid priority: {priority}",
                context=ErrorContext(
                    operation="validate_priority",
                    task_id=task_id,
                    additional_info={
                        "priority": priority,
                        "valid_priorities": [p.value for p in Priority],
                    },
                ),
            )

    def check_agent_skills(
        self,
        task_id: str,
        agent_id: str,
        required_skills: List[str],
    ) -> Dict[str, Any]:
        """
        Check if agent skills match task requirements (warning only).

        Parameters
        ----------
        task_id : str
            The task ID.
        agent_id : str
            The agent ID.
        required_skills : List[str]
            Required skills for the task.

        Returns
        -------
        Dict[str, Any]
            Result with match status and missing skills.

        Examples
        --------
        >>> result = validator.check_agent_skills("T-1", "agent-1", ["python", "api"])
        >>> if not result["match"]:
        ...     print(f"Warning: Missing skills {result['missing_skills']}")
        """
        agent = self.state.agents.get(agent_id)
        if not agent:
            return {
                "match": False,
                "missing_skills": required_skills,
                "warning": f"Agent {agent_id} not found",
            }

        agent_skills = set(agent.skills)
        required_set = set(required_skills)
        missing = required_set - agent_skills

        return {
            "match": len(missing) == 0,
            "missing_skills": list(missing),
            "warning": None if len(missing) == 0 else f"Agent missing skills: {missing}",
        }


class ProjectValidator:
    """
    Validates project operations.

    Examples
    --------
    >>> validator = ProjectValidator(state)
    >>> validator.validate_project_exists(project_id)
    """

    def __init__(self, state: Any):
        """Initialize project validator."""
        self.state = state

    def validate_project_exists(
        self,
        project_id: str,
        operation: str = "feature_creation",
    ) -> None:
        """
        Validate that project exists.

        Parameters
        ----------
        project_id : str
            The project ID.
        operation : str
            The operation being performed.

        Raises
        ------
        ValidationError
            If project doesn't exist.

        Examples
        --------
        >>> validator.validate_project_exists("proj-1", "feature_creation")
        """
        if project_id not in self.state.projects:
            raise ValidationError(
                f"Project {project_id} does not exist",
                context=ErrorContext(
                    operation=operation,
                    project_id=project_id,
                ),
            )

    def validate_feature_exists(
        self,
        feature_id: str,
        operation: str = "task_creation",
    ) -> None:
        """
        Validate that feature exists.

        Parameters
        ----------
        feature_id : str
            The feature ID.
        operation : str
            The operation being performed.

        Raises
        ------
        ValidationError
            If feature doesn't exist.

        Examples
        --------
        >>> validator.validate_feature_exists("F-100", "task_creation")
        """
        if feature_id not in self.state.features:
            raise ValidationError(
                f"Feature {feature_id} does not exist",
                context=ErrorContext(
                    operation=operation,
                    feature_id=feature_id,
                ),
            )
```

#### **Step 2: Integrate Validators**

Update `src/marcus_mcp/tools/core.py` to use validators:

```python
"""Core MCP tools with validation."""

from src.core.validators import ProjectValidator, TaskValidator


async def request_next_task(
    agent_id: str,
    state: Any = None,
) -> Dict[str, Any]:
    """Request next task with validation."""
    task_validator = TaskValidator(state)

    # Validate agent exists (#119)
    task_validator.validate_agent_exists(agent_id, "request_next_task")

    # Find next task
    task = _find_next_task(agent_id, state)

    if not task:
        return {"success": False, "message": "No tasks available"}

    # Validate dependencies before assignment (#118)
    task_validator.validate_task_dependencies(task.id, task.dependencies)

    # Check dependencies are completed
    for dep_id in task.dependencies:
        dep_task = state.tasks[dep_id]
        if dep_task.status != TaskStatus.COMPLETED:
            return {
                "success": False,
                "message": f"Task {task.id} blocked by incomplete dependency {dep_id}",
            }

    # Check agent skills (#125 - warning only)
    required_skills = task.source_context.get("required_skills", [])
    if required_skills:
        skill_check = task_validator.check_agent_skills(
            task.id, agent_id, required_skills
        )
        if not skill_check["match"]:
            # Log warning but allow assignment
            print(f"Warning: {skill_check['warning']}")

    # Validate status transition (#122)
    task_validator.validate_status_transition(
        task.id, task.status, TaskStatus.IN_PROGRESS
    )

    # Assign task
    task.assigned_to = agent_id
    task.status = TaskStatus.IN_PROGRESS
    agent = state.agents[agent_id]
    agent.current_task_id = task.id

    # Broadcast event
    broadcaster = get_broadcaster()
    await broadcaster.broadcast(
        "task_assigned",
        {
            "task_id": task.id,
            "task_name": task.name,
            "agent_id": agent_id,
            "agent_name": agent.name,
        },
    )

    return {
        "success": True,
        "task": {
            "id": task.id,
            "name": task.name,
            "description": task.description,
            "estimated_hours": task.estimated_hours,
        },
    }


async def report_task_progress(
    agent_id: str,
    task_id: str,
    status: str,
    progress: int = 0,
    message: str = "",
    state: Any = None,
) -> Dict[str, Any]:
    """Report task progress with validation."""
    task_validator = TaskValidator(state)

    # Validate agent exists (#119)
    task_validator.validate_agent_exists(agent_id, "report_task_progress")

    # Get task
    task = state.tasks.get(task_id)
    if not task:
        raise ValidationError(
            f"Task {task_id} not found",
            context=ErrorContext(operation="report_task_progress", task_id=task_id),
        )

    # Validate status transition (#122)
    new_status = TaskStatus(status)
    task_validator.validate_status_transition(task.id, task.status, new_status)

    # Update status
    old_status = task.status
    task.status = new_status

    # Broadcast event
    broadcaster = get_broadcaster()
    await broadcaster.broadcast(
        "task_progress",
        {
            "task_id": task_id,
            "agent_id": agent_id,
            "old_status": old_status.value,
            "new_status": status,
            "progress": progress,
            "message": message,
        },
    )

    # If completed, update agent
    if new_status == TaskStatus.COMPLETED:
        agent = state.agents[agent_id]
        agent.current_task_id = None
        agent.task_history.append(task_id)

        await broadcaster.broadcast(
            "task_completed",
            {
                "task_id": task_id,
                "task_name": task.name,
                "agent_id": agent_id,
            },
        )

    return {"success": True, "message": f"Progress updated for {task_id}"}
```

Update `src/ai/advanced/prd/advanced_parser.py` to validate during task creation:

```python
"""Advanced PRD parser with validation."""

from src.core.validators import ProjectValidator, TaskValidator


async def _generate_detailed_task(
    self,
    task_id: str,
    epic_id: str,
    analysis: PRDAnalysis,
    constraints: ProjectConstraints,
    sequence: int,
) -> Task:
    """Generate detailed task with validation."""
    # ... existing task generation logic ...

    # Validate before creating task
    task_validator = TaskValidator(self.state)

    # Validate dependencies exist (#118)
    if task_dependencies:
        task_validator.validate_task_dependencies(task_id, task_dependencies)

    # Validate estimated hours (#123)
    task_validator.validate_estimated_hours(task_id, estimated_hours)

    # Validate priority (#124)
    priority_enum = task_validator.validate_priority(task_id, priority)

    # Create task
    task = Task(
        id=task_id,
        name=task_name,
        description=description,
        status=TaskStatus.TODO,
        priority=priority_enum,
        assigned_to=None,
        dependencies=task_dependencies,
        estimated_hours=estimated_hours,
        source_context=source_context,
    )

    return task
```

#### **Step 3: Create Tests**

Create `tests/unit/core/test_validators.py`:

```python
"""
Unit tests for validation framework.
"""

from unittest.mock import Mock

import pytest

from src.core.error_framework import ValidationError
from src.core.models import Agent, Priority, Task, TaskStatus
from src.core.validators import ProjectValidator, TaskValidator


class TestTaskValidator:
    """Test task validator"""

    @pytest.fixture
    def state(self):
        """Create mock state."""
        state = Mock()
        state.agents = {}
        state.tasks = {}
        state.projects = {}
        state.features = {}
        return state

    @pytest.fixture
    def validator(self, state):
        """Create task validator."""
        return TaskValidator(state)

    @pytest.mark.unit
    def test_validate_task_dependencies_success(self, validator, state):
        """Test successful dependency validation."""
        # Create dependency tasks
        state.tasks["T-1"] = Mock(id="T-1")
        state.tasks["T-2"] = Mock(id="T-2")

        # Should not raise
        validator.validate_task_dependencies("T-3", ["T-1", "T-2"])

    @pytest.mark.unit
    def test_validate_task_dependencies_missing(self, validator, state):
        """Test validation fails with missing dependencies."""
        state.tasks["T-1"] = Mock(id="T-1")

        with pytest.raises(ValidationError) as exc_info:
            validator.validate_task_dependencies("T-3", ["T-1", "T-2", "T-999"])

        assert "invalid dependencies" in str(exc_info.value)
        assert "T-2" in str(exc_info.value)
        assert "T-999" in str(exc_info.value)

    @pytest.mark.unit
    def test_validate_agent_exists_success(self, validator, state):
        """Test successful agent validation."""
        state.agents["agent-1"] = Mock(agent_id="agent-1")

        # Should not raise
        validator.validate_agent_exists("agent-1")

    @pytest.mark.unit
    def test_validate_agent_exists_missing(self, validator, state):
        """Test validation fails with missing agent."""
        with pytest.raises(ValidationError) as exc_info:
            validator.validate_agent_exists("agent-999")

        assert "does not exist" in str(exc_info.value)
        assert "agent-999" in str(exc_info.value)

    @pytest.mark.unit
    def test_validate_status_transition_valid(self, validator):
        """Test valid status transitions."""
        # TODO -> IN_PROGRESS (valid)
        validator.validate_status_transition(
            "T-1", TaskStatus.TODO, TaskStatus.IN_PROGRESS
        )

        # IN_PROGRESS -> COMPLETED (valid)
        validator.validate_status_transition(
            "T-1", TaskStatus.IN_PROGRESS, TaskStatus.COMPLETED
        )

        # IN_PROGRESS -> BLOCKED (valid)
        validator.validate_status_transition(
            "T-1", TaskStatus.IN_PROGRESS, TaskStatus.BLOCKED
        )

    @pytest.mark.unit
    def test_validate_status_transition_invalid(self, validator):
        """Test invalid status transitions."""
        # COMPLETED -> anything (invalid)
        with pytest.raises(ValidationError) as exc_info:
            validator.validate_status_transition(
                "T-1", TaskStatus.COMPLETED, TaskStatus.TODO
            )

        assert "Invalid status transition" in str(exc_info.value)

        # TODO -> COMPLETED (invalid, must go through IN_PROGRESS)
        with pytest.raises(ValidationError):
            validator.validate_status_transition(
                "T-1", TaskStatus.TODO, TaskStatus.COMPLETED
            )

    @pytest.mark.unit
    def test_validate_estimated_hours_valid(self, validator):
        """Test valid estimated hours."""
        validator.validate_estimated_hours("T-1", 4.0)
        validator.validate_estimated_hours("T-1", 0.5)
        validator.validate_estimated_hours("T-1", 100.0)

    @pytest.mark.unit
    def test_validate_estimated_hours_invalid(self, validator):
        """Test invalid estimated hours."""
        with pytest.raises(ValidationError) as exc_info:
            validator.validate_estimated_hours("T-1", 0.0)

        assert "invalid estimated hours" in str(exc_info.value)

        with pytest.raises(ValidationError):
            validator.validate_estimated_hours("T-1", -5.0)

    @pytest.mark.unit
    def test_validate_priority_enum(self, validator):
        """Test priority validation with enum."""
        result = validator.validate_priority("T-1", Priority.HIGH)
        assert result == Priority.HIGH

    @pytest.mark.unit
    def test_validate_priority_string(self, validator):
        """Test priority validation with string."""
        result = validator.validate_priority("T-1", "high")
        assert result == Priority.HIGH

        result = validator.validate_priority("T-1", "medium")
        assert result == Priority.MEDIUM

    @pytest.mark.unit
    def test_validate_priority_invalid(self, validator):
        """Test invalid priority."""
        with pytest.raises(ValidationError) as exc_info:
            validator.validate_priority("T-1", "invalid")

        assert "invalid priority" in str(exc_info.value)

    @pytest.mark.unit
    def test_check_agent_skills_match(self, validator, state):
        """Test agent skills check with match."""
        agent = Mock(agent_id="agent-1", skills=["python", "api", "testing"])
        state.agents["agent-1"] = agent

        result = validator.check_agent_skills("T-1", "agent-1", ["python", "api"])

        assert result["match"] is True
        assert len(result["missing_skills"]) == 0
        assert result["warning"] is None

    @pytest.mark.unit
    def test_check_agent_skills_missing(self, validator, state):
        """Test agent skills check with missing skills."""
        agent = Mock(agent_id="agent-1", skills=["python"])
        state.agents["agent-1"] = agent

        result = validator.check_agent_skills("T-1", "agent-1", ["python", "api", "docker"])

        assert result["match"] is False
        assert set(result["missing_skills"]) == {"api", "docker"}
        assert "missing skills" in result["warning"].lower()


class TestProjectValidator:
    """Test project validator"""

    @pytest.fixture
    def state(self):
        """Create mock state."""
        state = Mock()
        state.projects = {}
        state.features = {}
        return state

    @pytest.fixture
    def validator(self, state):
        """Create project validator."""
        return ProjectValidator(state)

    @pytest.mark.unit
    def test_validate_project_exists_success(self, validator, state):
        """Test successful project validation."""
        state.projects["proj-1"] = Mock(project_id="proj-1")

        # Should not raise
        validator.validate_project_exists("proj-1")

    @pytest.mark.unit
    def test_validate_project_exists_missing(self, validator, state):
        """Test validation fails with missing project."""
        with pytest.raises(ValidationError) as exc_info:
            validator.validate_project_exists("proj-999")

        assert "does not exist" in str(exc_info.value)

    @pytest.mark.unit
    def test_validate_feature_exists_success(self, validator, state):
        """Test successful feature validation."""
        state.features["F-100"] = Mock(feature_id="F-100")

        # Should not raise
        validator.validate_feature_exists("F-100")

    @pytest.mark.unit
    def test_validate_feature_exists_missing(self, validator, state):
        """Test validation fails with missing feature."""
        with pytest.raises(ValidationError) as exc_info:
            validator.validate_feature_exists("F-999")

        assert "does not exist" in str(exc_info.value)
```

Run tests:
```bash
pytest tests/unit/core/test_validators.py -v

# Should show:
# ============================== 18 passed in 0.45s ===============================
```

#### **Step 4: Create Integration Tests**

Create `tests/integration/e2e/test_validation_integration.py`:

```python
"""
Integration tests for validation in real workflows.
"""

import pytest

from src.core.error_framework import ValidationError
from src.core.models import Agent, Priority, Task, TaskStatus
from src.marcus_mcp.tools.core import register_agent, report_task_progress, request_next_task
from src.state.manager import StateManager


class TestValidationIntegration:
    """Integration tests for validation"""

    @pytest.fixture
    def state(self):
        """Create state manager."""
        return StateManager()

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_task_assignment_validates_agent_exists(self, state):
        """Test task assignment fails if agent doesn't exist."""
        # Create task
        task = Task(
            id="T-1",
            name="Test Task",
            description="Test",
            status=TaskStatus.TODO,
            priority=Priority.HIGH,
            assigned_to=None,
            dependencies=[],
            estimated_hours=2.0,
            source_context={},
        )
        state.tasks[task.id] = task

        # Try to assign to non-existent agent
        with pytest.raises(ValidationError) as exc_info:
            await request_next_task("agent-999", state=state)

        assert "does not exist" in str(exc_info.value)

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_task_assignment_validates_dependencies(self, state):
        """Test task assignment validates dependencies exist."""
        # Register agent
        await register_agent("agent-1", "Test Agent", "developer", state=state)

        # Create task with non-existent dependency
        task = Task(
            id="T-1",
            name="Test Task",
            description="Test",
            status=TaskStatus.TODO,
            priority=Priority.HIGH,
            assigned_to=None,
            dependencies=["T-999"],  # Non-existent
            estimated_hours=2.0,
            source_context={},
        )
        state.tasks[task.id] = task

        # Try to assign
        with pytest.raises(ValidationError) as exc_info:
            await request_next_task("agent-1", state=state)

        assert "invalid dependencies" in str(exc_info.value)
        assert "T-999" in str(exc_info.value)

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_status_transition_validation(self, state):
        """Test status transition validation."""
        # Register agent
        await register_agent("agent-1", "Test Agent", "developer", state=state)

        # Create and assign task
        task = Task(
            id="T-1",
            name="Test Task",
            description="Test",
            status=TaskStatus.TODO,
            priority=Priority.HIGH,
            assigned_to="agent-1",
            dependencies=[],
            estimated_hours=2.0,
            source_context={},
        )
        state.tasks[task.id] = task
        state.agents["agent-1"].current_task_id = task.id

        # Complete task
        await report_task_progress("agent-1", "T-1", "in_progress", state=state)
        await report_task_progress("agent-1", "T-1", "completed", state=state)

        # Try invalid transition (completed -> todo)
        with pytest.raises(ValidationError) as exc_info:
            await report_task_progress("agent-1", "T-1", "todo", state=state)

        assert "Invalid status transition" in str(exc_info.value)
```

Run integration tests:
```bash
pytest tests/integration/e2e/test_validation_integration.py -v
```

**Success Criteria**:
- âœ… Validation framework created (TaskValidator, ProjectValidator)
- âœ… All 7 validations implemented (#118-#125)
- âœ… Integrated with MCP tools and PRD parser
- âœ… 18 unit tests passing
- âœ… 3 integration tests passing
- âœ… Issues #118-125 resolved

---

### **Tuesday: Docker Deployment Enhancement**

**Goal**: Enhance Docker deployment with production-ready configuration, health checks, and deployment guides.

**Background**: Current Docker setup is minimal. For MVP release, we need:
- Multi-stage builds for smaller images
- Health checks for container orchestration
- Environment variable configuration
- Docker Compose for local development
- Deployment guides for common platforms

#### **Step 1: Create Multi-stage Dockerfile**

Create `Dockerfile`:

```dockerfile
# Multi-stage build for Marcus MVP
# Stage 1: Builder
FROM python:3.11-slim as builder

WORKDIR /app

# Install build dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .
COPY requirements-dev.txt .

# Install Python dependencies
RUN pip install --no-cache-dir --user -r requirements.txt

# Stage 2: Runtime
FROM python:3.11-slim

WORKDIR /app

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy Python packages from builder
COPY --from=builder /root/.local /root/.local

# Make sure scripts in .local are usable
ENV PATH=/root/.local/bin:$PATH

# Copy application code
COPY src/ ./src/
COPY pyproject.toml .
COPY README.md .

# Create directories for Marcus data
RUN mkdir -p /app/.marcus/logs/audit \
    /app/.marcus/logs/research \
    /app/.marcus/db \
    /app/workspaces

# Environment variables
ENV PYTHONUNBUFFERED=1
ENV MARCUS_HOME=/app/.marcus
ENV WORKSPACE_ROOT=/app/workspaces

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Expose ports
EXPOSE 8000

# Run Marcus MCP server
CMD ["python", "-m", "src.marcus_mcp.server"]
```

#### **Step 2: Create Docker Compose Configuration**

Create `docker-compose.yml`:

```yaml
version: '3.8'

services:
  marcus:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: marcus-mcp
    ports:
      - "8000:8000"
    volumes:
      # Persist Marcus data
      - marcus-data:/app/.marcus
      # Mount workspaces (for development)
      - ./workspaces:/app/workspaces
    environment:
      - MARCUS_HOME=/app/.marcus
      - WORKSPACE_ROOT=/app/workspaces
      - LOG_LEVEL=INFO
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  cato:
    image: marcus/cato:latest
    container_name: cato-dashboard
    ports:
      - "3000:3000"
    environment:
      - MARCUS_API_URL=http://marcus:8000
    depends_on:
      marcus:
        condition: service_healthy
    restart: unless-stopped

volumes:
  marcus-data:
    driver: local
```

Create `docker-compose.dev.yml` for development:

```yaml
version: '3.8'

services:
  marcus:
    build:
      context: .
      dockerfile: Dockerfile
      target: builder  # Use builder stage for dev dependencies
    volumes:
      # Mount source for live reload
      - ./src:/app/src:ro
      - ./tests:/app/tests:ro
      # Persist data
      - marcus-data:/app/.marcus
    environment:
      - LOG_LEVEL=DEBUG
      - PYTHONUNBUFFERED=1
    command: ["python", "-m", "uvicorn", "src.api.main:app", "--reload", "--host", "0.0.0.0", "--port", "8000"]

volumes:
  marcus-data:
```

#### **Step 3: Create Deployment Guides**

Create `docs/deployment/DOCKER.md`:

```markdown
# Docker Deployment Guide

## Quick Start

### Local Development

```bash
# Start Marcus with hot-reload
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up

# View logs
docker-compose logs -f marcus

# Stop
docker-compose down
```

### Production

```bash
# Build and start
docker-compose up -d

# Check health
docker-compose ps
curl http://localhost:8000/health

# View logs
docker-compose logs -f marcus

# Stop
docker-compose down
```

## Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `MARCUS_HOME` | Marcus data directory | `/app/.marcus` |
| `WORKSPACE_ROOT` | Workspaces directory | `/app/workspaces` |
| `LOG_LEVEL` | Logging level | `INFO` |
| `DATABASE_URL` | Kanban database URL | (optional) |
| `OPENAI_API_KEY` | OpenAI API key | (required) |
| `ANTHROPIC_API_KEY` | Anthropic API key | (required) |

### Secrets Management

Create `.env` file (never commit):

```bash
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
DATABASE_URL=postgresql://user:pass@host:5432/db
```

Load in docker-compose:

```yaml
services:
  marcus:
    env_file:
      - .env
```

## Deployment Platforms

### AWS ECS

1. **Build and push image**:
```bash
docker build -t marcus:latest .
docker tag marcus:latest 123456789012.dkr.ecr.us-east-1.amazonaws.com/marcus:latest
docker push 123456789012.dkr.ecr.us-east-1.amazonaws.com/marcus:latest
```

2. **Create task definition** (`ecs-task-def.json`):
```json
{
  "family": "marcus",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "512",
  "memory": "1024",
  "containerDefinitions": [
    {
      "name": "marcus",
      "image": "123456789012.dkr.ecr.us-east-1.amazonaws.com/marcus:latest",
      "portMappings": [
        {
          "containerPort": 8000,
          "protocol": "tcp"
        }
      ],
      "healthCheck": {
        "command": ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"],
        "interval": 30,
        "timeout": 5,
        "retries": 3,
        "startPeriod": 10
      },
      "environment": [
        {"name": "MARCUS_HOME", "value": "/app/.marcus"}
      ],
      "secrets": [
        {"name": "OPENAI_API_KEY", "valueFrom": "arn:aws:secretsmanager:..."}
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/marcus",
          "awslogs-region": "us-east-1",
          "awslogs-stream-prefix": "ecs"
        }
      }
    }
  ]
}
```

3. **Create service**:
```bash
aws ecs create-service \
  --cluster marcus-cluster \
  --service-name marcus-service \
  --task-definition marcus \
  --desired-count 1 \
  --launch-type FARGATE \
  --network-configuration "awsvpcConfiguration={subnets=[subnet-xxx],securityGroups=[sg-xxx],assignPublicIp=ENABLED}"
```

### Google Cloud Run

```bash
# Build and push
gcloud builds submit --tag gcr.io/PROJECT_ID/marcus

# Deploy
gcloud run deploy marcus \
  --image gcr.io/PROJECT_ID/marcus \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --set-env-vars MARCUS_HOME=/app/.marcus \
  --set-secrets OPENAI_API_KEY=openai-key:latest
```

### Kubernetes

Create `k8s/deployment.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: marcus
  labels:
    app: marcus
spec:
  replicas: 2
  selector:
    matchLabels:
      app: marcus
  template:
    metadata:
      labels:
        app: marcus
    spec:
      containers:
      - name: marcus
        image: marcus:latest
        ports:
        - containerPort: 8000
        env:
        - name: MARCUS_HOME
          value: /app/.marcus
        envFrom:
        - secretRef:
            name: marcus-secrets
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 10
        volumeMounts:
        - name: marcus-data
          mountPath: /app/.marcus
      volumes:
      - name: marcus-data
        persistentVolumeClaim:
          claimName: marcus-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: marcus-service
spec:
  selector:
    app: marcus
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

Deploy:
```bash
kubectl apply -f k8s/deployment.yaml
kubectl get pods
kubectl logs -f deployment/marcus
```

## Monitoring

### Health Checks

```bash
# Basic health
curl http://localhost:8000/health

# Detailed status
curl http://localhost:8000/api/cato/snapshot | jq .
```

### Logs

```bash
# Docker Compose
docker-compose logs -f marcus

# Docker
docker logs -f marcus-mcp

# Kubernetes
kubectl logs -f deployment/marcus
```

### Metrics

Marcus exposes metrics at `/metrics` (Prometheus format):

```bash
curl http://localhost:8000/metrics
```

## Troubleshooting

### Container won't start

```bash
# Check logs
docker-compose logs marcus

# Common issues:
# - Missing environment variables
# - Database connection failed
# - Port already in use
```

### Health check failing

```bash
# Check health endpoint directly
docker exec marcus-mcp curl http://localhost:8000/health

# Check application logs
docker logs marcus-mcp --tail 100
```

### Out of memory

Increase memory limit:

```yaml
services:
  marcus:
    deploy:
      resources:
        limits:
          memory: 2G
```

## Security

### Best Practices

1. **Don't commit secrets** - Use environment variables or secret managers
2. **Run as non-root** - Add to Dockerfile:
   ```dockerfile
   RUN useradd -m -u 1000 marcus
   USER marcus
   ```
3. **Scan for vulnerabilities**:
   ```bash
   docker scan marcus:latest
   ```
4. **Use specific base image versions** - Pin Python version
5. **Limit container capabilities**:
   ```yaml
   security_opt:
     - no-new-privileges:true
   cap_drop:
     - ALL
   ```

## Performance Tuning

### Resource Limits

```yaml
services:
  marcus:
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 512M
```

### Volume Performance

Use volumes instead of bind mounts for better performance:

```yaml
volumes:
  - marcus-data:/app/.marcus  # Fast
  # vs
  - ./.marcus:/app/.marcus    # Slower on Mac/Windows
```

## Backup & Recovery

### Backup Marcus Data

```bash
# Create backup
docker run --rm \
  -v marcus-data:/data \
  -v $(pwd):/backup \
  ubuntu tar czf /backup/marcus-backup.tar.gz /data

# Restore backup
docker run --rm \
  -v marcus-data:/data \
  -v $(pwd):/backup \
  ubuntu tar xzf /backup/marcus-backup.tar.gz -C /
```
```

#### **Step 4: Create .dockerignore**

Create `.dockerignore`:

```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
.venv/
.pytest_cache/
.coverage
htmlcov/

# IDE
.vscode/
.idea/
*.swp
*.swo

# Git
.git/
.gitignore

# Documentation
docs/
*.md
!README.md

# Tests
tests/
test_*.py

# CI/CD
.github/
.gitlab-ci.yml

# Local data
.marcus/
workspaces/
*.db
*.sqlite

# Secrets
.env
*.key
*.pem
credentials.json
```

#### **Step 5: Test Docker Deployment**

Create `tests/integration/deployment/test_docker.py`:

```python
"""
Integration tests for Docker deployment.
"""

import subprocess
import time

import pytest
import requests


class TestDockerDeployment:
    """Test Docker deployment"""

    @pytest.fixture(scope="class")
    def docker_container(self):
        """Start Marcus container for testing."""
        # Build image
        subprocess.run(
            ["docker", "build", "-t", "marcus:test", "."],
            check=True,
            capture_output=True,
        )

        # Start container
        subprocess.run(
            [
                "docker",
                "run",
                "-d",
                "--name",
                "marcus-test",
                "-p",
                "8000:8000",
                "marcus:test",
            ],
            check=True,
            capture_output=True,
        )

        # Wait for container to be healthy
        for _ in range(30):
            try:
                response = requests.get("http://localhost:8000/health", timeout=1)
                if response.status_code == 200:
                    break
            except requests.RequestException:
                pass
            time.sleep(1)

        yield

        # Cleanup
        subprocess.run(
            ["docker", "stop", "marcus-test"], check=False, capture_output=True
        )
        subprocess.run(
            ["docker", "rm", "marcus-test"], check=False, capture_output=True
        )

    @pytest.mark.integration
    @pytest.mark.docker
    def test_container_health(self, docker_container):
        """Test container health check."""
        response = requests.get("http://localhost:8000/health")

        assert response.status_code == 200
        assert response.json()["status"] == "healthy"

    @pytest.mark.integration
    @pytest.mark.docker
    def test_api_endpoints_accessible(self, docker_container):
        """Test API endpoints are accessible."""
        # Test CATO snapshot endpoint
        response = requests.get("http://localhost:8000/api/cato/snapshot")

        assert response.status_code == 200
        assert "snapshot" in response.json()
```

Run Docker tests:
```bash
pytest tests/integration/deployment/test_docker.py -v -m docker
```

**Success Criteria**:
- âœ… Multi-stage Dockerfile created
- âœ… Docker Compose configurations for dev and prod
- âœ… Deployment guides for AWS, GCP, Kubernetes
- âœ… Health checks configured
- âœ… Security best practices documented
- âœ… Docker integration tests passing

---

### **Wednesday: Documentation & Examples**

**Goal**: Create comprehensive user-facing documentation and example projects.

#### **Step 1: Update Main README**

Update `README.md`:

```markdown
# Marcus - Multi-Agent Resource Coordination and Understanding System

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)]()

Marcus is a BYOA (Bring Your Own Agent) multi-agent coordination platform that helps multiple AI agents work together on complex software projects. It provides workspace isolation, task orchestration, and real-time observability.

## âœ¨ Features

### ğŸ¯ Core Capabilities
- **Task Orchestration**: Intelligent task assignment with dependency management
- **Workspace Isolation**: Git worktree-based isolation for parallel development
- **Real-time Observability**: CATO dashboard for live system visualization
- **Kanban Integration**: Sync with Planka, GitHub Projects, Linear
- **Feature Context**: Automatic context aggregation for informed development

### ğŸ”¬ Research-Grade Telemetry
- **User Journey Tracking**: Identify workflow bottlenecks
- **MAS Behavior Logging**: Capture agent coordination patterns
- **Event Broadcasting**: Real-time updates via SSE

### ğŸš€ Production Ready
- **Comprehensive Validation**: Prevent runtime errors with built-in validators
- **Error Framework**: Structured error handling with context
- **Docker Deployment**: Production-ready containerization
- **Health Checks**: Built-in health monitoring

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/marcus.git
cd marcus

# Install dependencies
pip install -r requirements.txt

# Run tests
pytest
```

### Docker

```bash
# Start Marcus
docker-compose up -d

# Check health
curl http://localhost:8000/health

# View dashboard
open http://localhost:3000
```

### First Project

```python
from src.marcus_mcp.tools.core import create_project

# Create a new project
result = await create_project(
    description="Build a REST API for task management",
    project_name="task-api"
)

print(f"Project ID: {result['project_id']}")
print(f"Tasks created: {result['tasks_created']}")
```

## ğŸ“š Documentation

- **[Getting Started](docs/getting-started/QUICKSTART.md)** - First steps
- **[Architecture](docs/architecture/OVERVIEW.md)** - System design
- **[API Reference](docs/api/README.md)** - Complete API docs
- **[Deployment](docs/deployment/DOCKER.md)** - Production deployment
- **[Examples](examples/)** - Sample projects

### Key Guides
- [Configuration Guide](docs/configuration/CONFIG.md)
- [Telemetry System](docs/features/TELEMETRY_SYSTEM.md)
- [Feature Context](docs/features/FEATURE_CONTEXT.md)
- [Workspace Management](docs/features/WORKSPACE_ISOLATION.md)

## ğŸ¯ Use Cases

### 1. Multi-Agent Software Development
```python
# Register agents
await register_agent("builder", "Builder", "developer", skills=["python", "api"])
await register_agent("tester", "Tester", "qa", skills=["pytest", "testing"])

# Create project (tasks auto-created)
result = await create_project(
    description="Build a REST API with authentication",
    project_name="secure-api"
)

# Agents request and complete tasks
task = await request_next_task("builder")
# ... agent implements task ...
await report_task_progress("builder", task["task"]["id"], "completed")
```

### 2. Research on MAS Coordination
```python
from src.research.event_logger import ResearchEventLogger

logger = ResearchEventLogger()

# Query agent coordination patterns
events = logger.query_events(event_type="agent_coordination", hours=168)

# Analyze coordination strategies
patterns = {}
for event in events:
    coord_type = event.event_data["coordination_type"]
    patterns[coord_type] = patterns.get(coord_type, 0) + 1

print(f"Parallel: {patterns['parallel']}")
print(f"Sequential: {patterns['sequential']}")
```

### 3. Real-time System Monitoring
```javascript
// Connect to CATO event stream
const eventSource = new EventSource('http://localhost:8000/api/cato/events/stream');

eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);

    if (data.event_type === 'task_assigned') {
        console.log(`Task ${data.event_data.task_id} assigned to ${data.event_data.agent_id}`);
    }
};
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Your Agents                           â”‚
â”‚              (Claude, GPT-4, Custom Agents)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ MCP Protocol
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Marcus Core                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Task       â”‚  â”‚  Workspace   â”‚  â”‚   Feature    â”‚     â”‚
â”‚  â”‚ Orchestrator â”‚  â”‚  Manager     â”‚  â”‚   Context    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚          Telemetry & Event System                â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
             â–¼                            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Kanban Boards  â”‚          â”‚ CATO Dashboard â”‚
    â”‚ (Planka, etc)  â”‚          â”‚ (Real-time UI) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Configuration

Create `.marcus/config.yaml`:

```yaml
# Workspace settings
workspace:
  root: "./workspaces"
  isolation: "worktree"  # or "directory"

# Kanban integration
kanban:
  provider: "planka"  # or "github", "linear"
  api_url: "http://localhost:3000"
  credentials:
    username: "user"
    password: "pass"

# Telemetry
telemetry:
  audit_log_dir: ".marcus/logs/audit"
  research_log_dir: ".marcus/logs/research"
  event_retention_days: 30

# AI providers
ai:
  default_provider: "anthropic"
  providers:
    anthropic:
      api_key: "${ANTHROPIC_API_KEY}"
      model: "claude-sonnet-4-5"
    openai:
      api_key: "${OPENAI_API_KEY}"
      model: "gpt-4"
```

## ğŸ§ª Development

### Running Tests

```bash
# All tests
pytest

# Unit tests only
pytest tests/unit/ -v

# Integration tests
pytest tests/integration/ -v

# With coverage
pytest --cov=src --cov-report=html
```

### Code Quality

```bash
# Format code
black src/ tests/

# Sort imports
isort src/ tests/

# Type checking
mypy src/

# Linting
flake8 src/
```

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows

# Install dev dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install

# Run tests
pytest
```

## ğŸ“Š Project Status

- âœ… Core task orchestration
- âœ… Workspace isolation with git worktrees
- âœ… Kanban integration (Planka)
- âœ… Feature context aggregation
- âœ… Telemetry system
- âœ… CATO dashboard integration
- âœ… Validation framework
- âœ… Docker deployment
- ğŸš§ GitHub Projects integration (in progress)
- ğŸš§ Linear integration (in progress)

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built with [Model Context Protocol (MCP)](https://modelcontextprotocol.io)
- Inspired by research in Multi-Agent Systems
- CATO dashboard for real-time visualization

## ğŸ“ Support

- **Documentation**: [docs/](docs/)
- **Issues**: [GitHub Issues](https://github.com/yourusername/marcus/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/marcus/discussions)

---

**Made with â¤ï¸ for the multi-agent future**
```

#### **Step 2: Create Getting Started Guide**

Create `docs/getting-started/QUICKSTART.md`:

```markdown
# Quick Start Guide

Get up and running with Marcus in 5 minutes.

## Prerequisites

- Python 3.11+
- Git
- Docker (optional, for containerized deployment)

## Installation

### Option 1: Local Installation

```bash
# Clone repository
git clone https://github.com/yourusername/marcus.git
cd marcus

# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run tests to verify
pytest tests/unit/ -v
```

### Option 2: Docker

```bash
# Clone repository
git clone https://github.com/yourusername/marcus.git
cd marcus

# Start Marcus
docker-compose up -d

# Check status
docker-compose ps
curl http://localhost:8000/health
```

## Your First Project

### Step 1: Start Marcus MCP Server

```bash
# Local
python -m src.marcus_mcp.server

# Docker
# Already running via docker-compose
```

### Step 2: Register an Agent

```python
import asyncio
from src.marcus_mcp.tools.core import register_agent
from src.state.manager import StateManager

async def setup():
    state = StateManager()

    # Register your first agent
    result = await register_agent(
        agent_id="builder-1",
        name="Builder",
        role="developer",
        skills=["python", "api", "database"],
        state=state
    )

    print(f"Agent registered: {result}")

asyncio.run(setup())
```

### Step 3: Create a Project

```python
from src.marcus_mcp.tools.project import create_project

async def create_first_project():
    result = await create_project(
        description="""
        Build a REST API for a task management system with:
        - User authentication (JWT)
        - Create, read, update, delete tasks
        - Task categories and tags
        - Due dates and priorities
        """,
        project_name="task-api"
    )

    print(f"Project created: {result['project_id']}")
    print(f"Tasks: {result['tasks_created']}")
    print(f"Features: {len(result.get('features', []))}")

asyncio.run(create_first_project())
```

### Step 4: Assign Tasks to Agents

```python
from src.marcus_mcp.tools.core import request_next_task

async def work_on_tasks():
    # Agent requests next task
    result = await request_next_task(
        agent_id="builder-1",
        state=state
    )

    if result["success"]:
        task = result["task"]
        print(f"Working on: {task['name']}")
        print(f"Description: {task['description']}")

        # Agent would work on task here...

        # Report progress
        await report_task_progress(
            agent_id="builder-1",
            task_id=task["id"],
            status="completed",
            message="Implemented authentication with JWT",
            state=state
        )

asyncio.run(work_on_tasks())
```

## Viewing Progress

### Option 1: CATO Dashboard

Open http://localhost:3000 to see:
- Agent status and task assignments
- Task dependency graph
- Real-time event stream
- Project progress

### Option 2: API Endpoints

```bash
# System snapshot
curl http://localhost:8000/api/cato/snapshot | jq .

# Journey metrics
curl http://localhost:8000/api/cato/metrics/journey?hours=24 | jq .

# Agent detail
curl http://localhost:8000/api/cato/agent/builder-1 | jq .
```

## Next Steps

- **[Configuration Guide](../configuration/CONFIG.md)** - Customize Marcus
- **[Architecture Overview](../architecture/OVERVIEW.md)** - Understand the system
- **[API Reference](../api/README.md)** - Explore all tools
- **[Examples](../../examples/)** - Learn from sample projects

## Troubleshooting

### MCP Server Won't Start

```bash
# Check Python version
python --version  # Should be 3.11+

# Check dependencies
pip list | grep -E "(fastapi|anthropic|openai)"

# Check logs
tail -f .marcus/logs/audit/*.log
```

### Tasks Not Being Created

```bash
# Check AI provider API keys
echo $ANTHROPIC_API_KEY
echo $OPENAI_API_KEY

# Test AI provider
python -c "import anthropic; print(anthropic.Anthropic().models.list())"
```

### Docker Container Unhealthy

```bash
# Check logs
docker-compose logs marcus

# Check health endpoint
docker exec marcus-mcp curl http://localhost:8000/health

# Restart
docker-compose restart marcus
```

## Common Questions

**Q: Can I use my own agents?**
A: Yes! Marcus is BYOA (Bring Your Own Agent). Register any agent that can communicate via MCP.

**Q: How do I integrate with GitHub Projects?**
A: See [Kanban Integration Guide](../integration/KANBAN.md)

**Q: Can multiple agents work in parallel?**
A: Yes! Marcus uses git worktrees for workspace isolation, enabling parallel development.

**Q: How do I contribute?**
A: See [CONTRIBUTING.md](../../CONTRIBUTING.md)
```

#### **Step 3: Create Example Projects**

Create `examples/simple-api/README.md`:

```markdown
# Simple API Example

A complete example showing how to use Marcus to build a REST API.

## Overview

This example demonstrates:
- Creating a project from natural language
- Multiple agents working together
- Task orchestration with dependencies
- Feature-based development
- Real-time monitoring

## Running the Example

```bash
cd examples/simple-api
python run_example.py
```

## What Happens

1. **Project Creation**: Marcus analyzes the description and creates:
   - 8 tasks (setup, auth, CRUD endpoints, tests)
   - 3 features (authentication, task management, testing)
   - Task dependency graph

2. **Agent Assignment**: Two agents work in parallel:
   - **Builder**: Implements features
   - **Tester**: Writes and runs tests

3. **Progress Tracking**: Watch in CATO dashboard:
   - Real-time task updates
   - Agent coordination
   - Feature progress

## Code

See `run_example.py` for complete implementation.

## Expected Output

```
Creating project...
âœ“ Project created: proj_simple_api
âœ“ Tasks created: 8
âœ“ Features: 3

Registering agents...
âœ“ Agent registered: builder-1
âœ“ Agent registered: tester-1

Starting work...
[Builder] Working on: Set up FastAPI project structure
[Builder] âœ“ Completed in 2m 15s
[Tester] Working on: Write unit tests for authentication
[Tester] âœ“ Completed in 1m 30s
...

Project completed in 15m 45s
All tests passing âœ“
```
```

Create `examples/simple-api/run_example.py`:

```python
"""
Simple API Example - Complete workflow demonstration.
"""

import asyncio
from datetime import datetime

from src.marcus_mcp.tools.core import (
    register_agent,
    report_task_progress,
    request_next_task,
)
from src.marcus_mcp.tools.project import create_project
from src.state.manager import StateManager


async def run_example():
    """Run simple API example."""
    print("=" * 60)
    print("Marcus Simple API Example")
    print("=" * 60)
    print()

    state = StateManager()
    start_time = datetime.now()

    # Step 1: Create project
    print("ğŸ“¦ Creating project...")
    project_result = await create_project(
        description="""
        Build a REST API for task management with:
        - User authentication (JWT tokens)
        - CRUD operations for tasks
        - Task categories and priorities
        - Unit and integration tests
        """,
        project_name="simple-api",
    )

    print(f"âœ“ Project created: {project_result['project_id']}")
    print(f"âœ“ Tasks created: {project_result['tasks_created']}")
    print(f"âœ“ Features: {len(project_result.get('features', []))}")
    print()

    # Step 2: Register agents
    print("ğŸ¤– Registering agents...")
    await register_agent(
        "builder-1", "Builder", "developer", skills=["python", "fastapi", "api"], state=state
    )
    await register_agent(
        "tester-1", "Tester", "qa", skills=["pytest", "testing"], state=state
    )
    print("âœ“ Agents registered")
    print()

    # Step 3: Simulate agent work
    print("âš¡ Starting work...")
    print()

    agents = ["builder-1", "tester-1"]
    completed_tasks = 0
    total_tasks = project_result["tasks_created"]

    while completed_tasks < total_tasks:
        # Each agent requests and completes a task
        for agent_id in agents:
            result = await request_next_task(agent_id, state=state)

            if result["success"]:
                task = result["task"]
                print(f"[{agent_id}] Working on: {task['name']}")

                # Simulate work (in real scenario, agent would implement)
                await asyncio.sleep(1)

                # Complete task
                await report_task_progress(
                    agent_id, task["id"], "completed", progress=100, state=state
                )
                completed_tasks += 1

                print(f"[{agent_id}] âœ“ Completed")
                print()

    # Summary
    duration = (datetime.now() - start_time).total_seconds()
    print("=" * 60)
    print(f"âœ“ Project completed in {duration:.0f}s")
    print(f"âœ“ All {total_tasks} tasks completed")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(run_example())
```

#### **Step 4: Create API Reference**

Create `docs/api/README.md`:

```markdown
# API Reference

Complete reference for Marcus MCP tools and APIs.

## MCP Tools

### Agent Management

#### `register_agent`

Register a new agent with Marcus.

**Parameters:**
- `agent_id` (str): Unique agent identifier
- `name` (str): Human-readable agent name
- `role` (str): Agent role (e.g., "developer", "qa", "architect")
- `skills` (list[str]): Agent capabilities

**Returns:**
```json
{
  "success": true,
  "agent_id": "agent-1",
  "message": "Agent Builder registered"
}
```

**Example:**
```python
await register_agent(
    "builder-1",
    "Builder",
    "developer",
    skills=["python", "api"]
)
```

#### `request_next_task`

Request the next available task for an agent.

**Parameters:**
- `agent_id` (str): Agent requesting task

**Returns:**
```json
{
  "success": true,
  "task": {
    "id": "T-1",
    "name": "Implement authentication",
    "description": "...",
    "estimated_hours": 4.0
  }
}
```

### Project Management

#### `create_project`

Create a new project from natural language description.

**Parameters:**
- `description` (str): Project requirements in natural language
- `project_name` (str): Project name
- `options` (dict, optional): Configuration options

**Returns:**
```json
{
  "success": true,
  "project_id": "proj_123",
  "tasks_created": 15,
  "features": [...],
  "board_info": {...}
}
```

**Example:**
```python
result = await create_project(
    description="Build a REST API for task management",
    project_name="task-api",
    options={
        "mode": "new_project",
        "complexity": "standard"
    }
)
```

### Feature Context

#### `get_feature_context`

Get complete context for a feature.

**Parameters:**
- `feature_id` (str): Feature identifier

**Returns:**
```json
{
  "success": true,
  "context": "# Feature: Authentication\n...",
  "summary": {...},
  "artifacts": [...],
  "decisions": [...],
  "commits": [...]
}
```

#### `get_feature_status`

Get current status of a feature.

**Parameters:**
- `feature_id` (str): Feature identifier

**Returns:**
```json
{
  "success": true,
  "status": {
    "feature_id": "F-100",
    "status": "in_progress",
    "progress_percentage": 65.0,
    "tasks_total": 5,
    "tasks_completed": 3
  }
}
```

## REST API

### CATO Endpoints

#### `GET /api/cato/snapshot`

Get current system snapshot.

**Response:**
```json
{
  "success": true,
  "snapshot": {
    "nodes": {
      "agents": [...],
      "tasks": [...],
      "projects": [...],
      "features": [...]
    },
    "edges": {...},
    "metrics": {...}
  }
}
```

#### `GET /api/cato/events/stream`

Server-sent events stream for real-time updates.

**Query Parameters:**
- `event_types` (list[str], optional): Filter event types

**Example:**
```javascript
const eventSource = new EventSource(
  '/api/cato/events/stream?event_types=task_assigned,task_completed'
);

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);
  console.log('Event:', data);
};
```

#### `GET /api/cato/metrics/journey`

Get user journey metrics.

**Query Parameters:**
- `milestone_type` (str, optional): Filter by milestone type
- `hours` (int): Lookback period (default: 24, max: 168)

**Response:**
```json
{
  "success": true,
  "metrics": {
    "total_milestones": 100,
    "completed": 85,
    "failed": 5,
    "completion_rate": 0.85,
    "avg_duration_seconds": 45.2
  }
}
```

For complete API documentation, see individual endpoint guides:
- [Agent Management](./AGENT_MANAGEMENT.md)
- [Project Management](./PROJECT_MANAGEMENT.md)
- [Feature Context](./FEATURE_CONTEXT.md)
- [CATO API](./CATO_API.md)
```

**Success Criteria**:
- âœ… README updated with comprehensive overview
- âœ… Quick start guide created
- âœ… Example project with working code
- âœ… API reference documentation
- âœ… All documentation reviewed and tested

---

### **Thursday: Final Testing & Bug Fixes**

**Goal**: Run comprehensive test suite, fix bugs, and ensure system stability.

#### **Step 1: Full System Test Suite**

Run all tests:

```bash
# Unit tests
pytest tests/unit/ -v --cov=src --cov-report=html

# Integration tests
pytest tests/integration/ -v

# E2E tests
pytest tests/integration/e2e/ -v

# Performance tests
pytest tests/performance/ -v

# Docker tests
pytest tests/integration/deployment/test_docker.py -v -m docker
```

#### **Step 2: Manual Testing Checklist**

Create `docs/testing/MANUAL_TEST_CHECKLIST.md`:

```markdown
# Manual Testing Checklist

## Pre-Release Testing

### Core Functionality

- [ ] **Project Creation**
  - [ ] Create project from simple description
  - [ ] Create project from complex description (10+ features)
  - [ ] Verify tasks created correctly
  - [ ] Check dependency graph is valid
  - [ ] Confirm Kanban board sync

- [ ] **Agent Operations**
  - [ ] Register agent successfully
  - [ ] Agent requests task (gets appropriate task)
  - [ ] Agent reports progress
  - [ ] Agent completes task
  - [ ] Multiple agents work in parallel

- [ ] **Feature Context**
  - [ ] Get feature context (complete data)
  - [ ] Feature status shows correct progress
  - [ ] Artifacts appear in context
  - [ ] Decisions logged correctly
  - [ ] Git commits tracked

- [ ] **Workspace Isolation**
  - [ ] Create workspace with git worktree
  - [ ] Multiple workspaces in parallel
  - [ ] Merge worktree after completion
  - [ ] Cleanup on failure

### Telemetry & Monitoring

- [ ] **CATO Dashboard**
  - [ ] Dashboard loads successfully
  - [ ] Snapshot shows current state
  - [ ] Event stream connects
  - [ ] Real-time updates appear
  - [ ] Agent detail view works

- [ ] **Journey Tracking**
  - [ ] Milestones recorded
  - [ ] Completion rates calculated
  - [ ] Bottlenecks identified

- [ ] **Research Logging**
  - [ ] Events logged correctly
  - [ ] Query interface works
  - [ ] Metrics aggregated properly

### Validation

- [ ] **Error Handling**
  - [ ] Invalid agent ID rejected
  - [ ] Missing dependencies detected
  - [ ] Invalid status transitions blocked
  - [ ] Helpful error messages shown

- [ ] **Edge Cases**
  - [ ] Empty project description handled
  - [ ] Circular dependencies detected
  - [ ] Duplicate agent IDs prevented
  - [ ] Invalid skill requirements warned

### Performance

- [ ] **Load Testing**
  - [ ] 100 tasks created in < 10s
  - [ ] 10 agents work in parallel
  - [ ] Event broadcast handles 1000 events/sec
  - [ ] No memory leaks after 1 hour

### Deployment

- [ ] **Docker**
  - [ ] Build succeeds
  - [ ] Container starts healthy
  - [ ] Health checks pass
  - [ ] API endpoints accessible
  - [ ] Logs visible

- [ ] **Configuration**
  - [ ] Environment variables loaded
  - [ ] Config file parsed correctly
  - [ ] Secrets managed securely

## Browser Compatibility (CATO)

- [ ] Chrome (latest)
- [ ] Firefox (latest)
- [ ] Safari (latest)
- [ ] Edge (latest)

## Platform Testing

- [ ] Linux (Ubuntu 22.04)
- [ ] macOS (Ventura+)
- [ ] Windows 11 (via WSL2)
```

#### **Step 3: Bug Fixing**

Track and fix any discovered issues:

```bash
# Run full test suite and capture failures
pytest -v --tb=short > test_results.txt 2>&1

# Review failures
cat test_results.txt | grep FAILED

# Fix issues and re-test
pytest tests/unit/specific_test.py -v

# Verify fix didn't break anything
pytest -v
```

#### **Step 4: Performance Validation**

Create `tests/performance/test_system_performance.py`:

```python
"""
System-wide performance tests.
"""

import asyncio
import time

import pytest

from src.marcus_mcp.tools.core import register_agent, request_next_task
from src.marcus_mcp.tools.project import create_project
from src.state.manager import StateManager


class TestSystemPerformance:
    """System performance tests"""

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_project_creation_performance(self):
        """Test project creation completes in reasonable time."""
        start_time = time.time()

        result = await create_project(
            description="""
            Build a complete e-commerce platform with:
            - User authentication
            - Product catalog
            - Shopping cart
            - Payment processing
            - Order management
            - Admin dashboard
            """,
            project_name="ecommerce-perf-test",
        )

        duration = time.time() - start_time

        assert duration < 30, f"Project creation took {duration}s (max 30s)"
        assert result["tasks_created"] > 10

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_parallel_agent_throughput(self):
        """Test multiple agents working in parallel."""
        state = StateManager()

        # Create project with many tasks
        await create_project(
            description="Build 20 simple REST API endpoints",
            project_name="throughput-test",
        )

        # Register 10 agents
        agents = []
        for i in range(10):
            agent_id = f"agent-{i}"
            await register_agent(agent_id, f"Agent {i}", "developer", state=state)
            agents.append(agent_id)

        # All agents request tasks in parallel
        start_time = time.time()

        tasks = await asyncio.gather(
            *[request_next_task(agent_id, state=state) for agent_id in agents]
        )

        duration = time.time() - start_time

        # Should complete in < 5s
        assert duration < 5, f"Parallel task assignment took {duration}s (max 5s)"

        # All should get tasks
        successful = sum(1 for t in tasks if t["success"])
        assert successful == 10
```

Run performance tests:
```bash
pytest tests/performance/test_system_performance.py -v -s
```

#### **Step 5: Code Quality Check**

```bash
# Run all quality checks
black --check src/ tests/
isort --check src/ tests/
flake8 src/ tests/
mypy src/
bandit -r src/

# Fix issues
black src/ tests/
isort src/ tests/
```

**Success Criteria**:
- âœ… All unit tests passing (100%)
- âœ… All integration tests passing (100%)
- âœ… Performance tests meet targets
- âœ… Manual testing checklist completed
- âœ… All critical bugs fixed
- âœ… Code quality checks pass

---

### **Friday: MVP Release Preparation**

**Goal**: Final preparations for MVP release including versioning, changelog, and release notes.

#### **Step 1: Version Bump**

Update `pyproject.toml`:

```toml
[project]
name = "marcus"
version = "0.1.0"  # MVP release
description = "Multi-Agent Resource Coordination and Understanding System"
authors = [
    {name = "Your Name", email = "your.email@example.com"}
]
requires-python = ">=3.11"
license = {text = "MIT"}

[project.urls]
Homepage = "https://github.com/yourusername/marcus"
Documentation = "https://github.com/yourusername/marcus/docs"
Repository = "https://github.com/yourusername/marcus"
Issues = "https://github.com/yourusername/marcus/issues"
```

#### **Step 2: Create CHANGELOG**

Create `CHANGELOG.md`:

```markdown
# Changelog

All notable changes to Marcus will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [0.1.0] - 2025-01-15

### Added

#### Core Features
- **Task Orchestration**: Intelligent task assignment with dependency management
- **Workspace Isolation**: Git worktree-based isolation for parallel agent development
- **Feature Context**: Automatic context aggregation with git commit tracking
- **Project Creation**: Natural language to project structure with AI-powered task generation

#### Integrations
- **Kanban Boards**: Planka integration with bidirectional sync
- **CATO Dashboard**: Real-time visualization with network graphs
- **MCP Protocol**: Full MCP tool implementation for agent communication

#### Telemetry & Observability
- **User Journey Tracking**: Milestone tracking with completion rates and bottleneck identification
- **Research Event Logging**: MAS behavior pattern capture for research
- **Event Broadcasting**: Pub-sub pattern with SSE streaming for real-time updates
- **Comprehensive Metrics**: Journey, research, and performance metrics APIs

#### Reliability
- **Validation Framework**: 7 core validations (task dependencies, agent existence, status transitions, etc.)
- **Error Framework**: Structured error handling with context and retry strategies
- **Health Checks**: Built-in health monitoring for container orchestration

#### Deployment
- **Docker Support**: Multi-stage Dockerfile with production optimizations
- **Docker Compose**: Development and production configurations
- **Deployment Guides**: AWS ECS, Google Cloud Run, Kubernetes instructions

### Documentation
- Comprehensive README with quick start
- Getting started guide
- API reference documentation
- Example projects with working code
- Deployment guides for major platforms
- Telemetry system documentation
- Feature context documentation

### Testing
- 100+ unit tests with 95%+ coverage
- Integration tests for core workflows
- End-to-end tests for complete scenarios
- Performance tests for throughput validation
- Docker deployment tests

### Performance
- Project creation: < 30s for complex projects (20+ tasks)
- Event throughput: 1000+ events/sec
- Parallel agents: 10 agents working simultaneously
- SSE latency: < 10ms

## [Unreleased]

### Planned
- GitHub Projects integration
- Linear integration
- Multi-provider AI support (OpenAI, Anthropic, local models)
- Advanced scheduling algorithms
- Anomaly detection in agent behavior
- A/B testing framework for coordination strategies

---

## Release Notes Format

### Added
New features

### Changed
Changes to existing functionality

### Deprecated
Features that will be removed in future releases

### Removed
Features that have been removed

### Fixed
Bug fixes

### Security
Security improvements
```

#### **Step 3: Create Release Notes**

Create `docs/releases/v0.1.0.md`:

```markdown
# Marcus v0.1.0 - MVP Release

**Release Date**: January 15, 2025

We're excited to announce the first public release of Marcus - a BYOA (Bring Your Own Agent) multi-agent coordination platform!

## ğŸ‰ Highlights

### Task Orchestration
Marcus intelligently assigns tasks to agents based on skills, dependencies, and workload. The dependency-aware scheduler ensures tasks are completed in the correct order while maximizing parallelization.

### Workspace Isolation
Using git worktrees, Marcus enables multiple agents to work on the same codebase in parallel without conflicts. Each agent gets an isolated workspace with automatic merging when tasks complete.

### Real-time Observability
The CATO dashboard provides live visualization of your multi-agent system:
- Network graph of agents, tasks, and dependencies
- Real-time event stream
- Journey analytics to identify bottlenecks
- Research-grade logging for MAS studies

### Production Ready
- Comprehensive validation prevents runtime errors
- Structured error handling with retry strategies
- Docker deployment with health checks
- 95%+ test coverage

## ğŸ“¦ What's Included

### Core Components
- MCP server with 15+ tools
- Task orchestration engine
- Workspace manager with git integration
- Feature context aggregation
- Kanban board sync (Planka)

### Telemetry System
- User journey tracking
- MAS behavior logging
- Event broadcasting with SSE
- Metrics APIs

### Documentation
- Quick start guide
- API reference
- Deployment guides (AWS, GCP, K8s)
- Example projects

## ğŸš€ Getting Started

```bash
# Docker (recommended)
git clone https://github.com/yourusername/marcus.git
cd marcus
docker-compose up -d
open http://localhost:3000

# Local installation
pip install -r requirements.txt
python -m src.marcus_mcp.server
```

See [Quick Start Guide](../getting-started/QUICKSTART.md) for detailed instructions.

## ğŸ“Š Performance

- **Project Creation**: < 30s for 20+ tasks
- **Event Throughput**: 1,000+ events/sec
- **Parallel Agents**: 10 agents simultaneously
- **Test Coverage**: 95%+

## ğŸ› Known Issues

- [ ] GitHub Projects integration not yet implemented (Issue #150)
- [ ] Linear integration not yet implemented (Issue #151)
- [ ] Workspace cleanup can be slow for large repositories (Issue #152)

See [GitHub Issues](https://github.com/yourusername/marcus/issues) for complete list.

## ğŸ”® What's Next

**v0.2.0** (February 2025):
- GitHub Projects integration
- Linear integration
- Enhanced AI provider support
- Advanced scheduling algorithms

**v0.3.0** (March 2025):
- Anomaly detection
- Predictive analytics
- A/B testing framework
- Export to external analytics platforms

See [Roadmap](../ROADMAP.md) for details.

## ğŸ™ Acknowledgments

Special thanks to:
- Early adopters and testers
- Contributors to the codebase
- The MCP community

## ğŸ“ Support

- **Documentation**: [docs/](../getting-started/QUICKSTART.md)
- **Issues**: [GitHub Issues](https://github.com/yourusername/marcus/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/marcus/discussions)

---

**Ready to coordinate your agents?** [Get Started â†’](../getting-started/QUICKSTART.md)
```

#### **Step 4: Final Pre-Release Checks**

Create pre-release checklist:

```bash
#!/bin/bash
# pre_release_check.sh

echo "Marcus MVP Pre-Release Checklist"
echo "================================="
echo ""

# 1. Tests
echo "[ ] Running test suite..."
pytest -v --cov=src --cov-report=term-missing
if [ $? -eq 0 ]; then
    echo "âœ“ All tests passing"
else
    echo "âœ— Tests failing - fix before release"
    exit 1
fi

# 2. Code quality
echo ""
echo "[ ] Checking code quality..."
black --check src/ tests/
isort --check src/ tests/
flake8 src/ tests/
mypy src/

if [ $? -eq 0 ]; then
    echo "âœ“ Code quality checks pass"
else
    echo "âœ— Code quality issues - fix before release"
    exit 1
fi

# 3. Documentation
echo ""
echo "[ ] Checking documentation..."
if [ -f "README.md" ] && [ -f "CHANGELOG.md" ] && [ -f "docs/getting-started/QUICKSTART.md" ]; then
    echo "âœ“ Documentation complete"
else
    echo "âœ— Documentation incomplete"
    exit 1
fi

# 4. Docker
echo ""
echo "[ ] Testing Docker build..."
docker build -t marcus:test .
if [ $? -eq 0 ]; then
    echo "âœ“ Docker build successful"
else
    echo "âœ— Docker build failed"
    exit 1
fi

# 5. Version
echo ""
echo "[ ] Checking version..."
VERSION=$(grep "version =" pyproject.toml | cut -d'"' -f2)
echo "Current version: $VERSION"

# 6. Git status
echo ""
echo "[ ] Checking git status..."
if [ -z "$(git status --porcelain)" ]; then
    echo "âœ“ Working directory clean"
else
    echo "âœ— Uncommitted changes detected"
    git status --short
    exit 1
fi

echo ""
echo "================================="
echo "âœ“ All pre-release checks passed!"
echo "Ready to create release v$VERSION"
echo "================================="
```

Run pre-release checks:
```bash
chmod +x pre_release_check.sh
./pre_release_check.sh
```

#### **Step 5: Create Release**

```bash
# Tag release
git tag -a v0.1.0 -m "Marcus MVP Release v0.1.0"

# Push to GitHub
git push origin develop
git push origin v0.1.0

# Create GitHub release
gh release create v0.1.0 \
  --title "Marcus v0.1.0 - MVP Release" \
  --notes-file docs/releases/v0.1.0.md \
  --draft

# Build and push Docker image
docker build -t marcus:0.1.0 -t marcus:latest .
docker tag marcus:0.1.0 yourusername/marcus:0.1.0
docker tag marcus:latest yourusername/marcus:latest
docker push yourusername/marcus:0.1.0
docker push yourusername/marcus:latest
```

**Success Criteria**:
- âœ… Version bumped to 0.1.0
- âœ… CHANGELOG.md created
- âœ… Release notes written
- âœ… All pre-release checks pass
- âœ… Git tag created
- âœ… GitHub release published
- âœ… Docker images pushed
- âœ… Documentation complete

---

## **Implementation Complete!**

ğŸ‰ **Congratulations!** The 6-week MVP implementation plan is now complete.

### Summary

**Weeks 1-3**: Foundation
- Configuration centralization (#68)
- Workspace isolation with git worktrees
- Feature context aggregation
- Git commit tracking

**Week 4**: Feature Context
- FeatureContextBuilder
- get_feature_context and get_feature_status tools
- Automatic context injection
- Complete test coverage

**Week 5**: Telemetry & CATO
- User journey tracking
- Research event logging
- CATO dashboard integration
- Real-time event broadcasting

**Week 6**: Production Readiness
- Core validations (#118-125)
- Docker deployment
- Comprehensive documentation
- MVP release

### Metrics

- **Files Added**: 50+
- **Lines of Code**: ~15,000
- **Tests Written**: 100+
- **Test Coverage**: 95%+
- **Documentation Pages**: 20+

### Ready for Release

Marcus is now ready for MVP release with:
- âœ… Core functionality complete
- âœ… Production-ready deployment
- âœ… Comprehensive testing
- âœ… Complete documentation
- âœ… Docker support
- âœ… Real-time monitoring

**Next Steps**: See `docs/ROADMAP.md` for v0.2.0+ planning.
