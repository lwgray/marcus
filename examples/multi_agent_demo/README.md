# Marcus Multi-Agent Development Demo

This demo showcases Marcus's ability to coordinate multiple agents working in parallel to build a production-quality REST API, with automated validation of quality metrics.

## Project Overview

**Goal**: Build a Task Management API that demonstrates:
- ✅ Multi-agent parallel development
- ✅ High code quality (80%+ test coverage, zero type errors)
- ✅ 100% API specification compliance
- ✅ Reduced time-to-completion vs single-agent development

## What Gets Built

A complete REST API with 15 endpoints across 5 feature areas:

1. **Authentication** (2 endpoints)
   - User registration with JWT tokens
   - User login with credential validation

2. **User Management** (2 endpoints)
   - Get current user profile
   - Update user profile

3. **Project Management** (5 endpoints)
   - List/create/read/update/delete projects
   - Filtering and pagination

4. **Task Management** (5 endpoints)
   - List/create/read/update/delete tasks
   - Task assignment
   - Filtering by status, priority, assignee

5. **Comments** (2 endpoints)
   - List comments on tasks
   - Add new comments

## Architecture

```
examples/multi_agent_demo/
├── task_management_api_spec.yaml    # OpenAPI specification (source of truth)
├── validate_api.py                  # Validation suite
├── requirements.txt                 # Dependencies
├── README.md                        # This file
└── implementation/                  # Generated by agents
    ├── app/
    │   ├── main.py                 # FastAPI app
    │   ├── models.py               # SQLAlchemy models
    │   ├── schemas.py              # Pydantic schemas
    │   ├── auth.py                 # Authentication logic
    │   ├── database.py             # Database setup
    │   └── routers/                # API route handlers
    │       ├── auth.py
    │       ├── users.py
    │       ├── projects.py
    │       ├── tasks.py
    │       └── comments.py
    └── tests/                      # Test suite
        ├── test_auth.py
        ├── test_users.py
        ├── test_projects.py
        ├── test_tasks.py
        └── test_comments.py
```

## Demo Workflow

### Phase 1: Setup (Manual)
```bash
cd examples/multi_agent_demo
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Phase 2: Generate Project with Marcus
```python
# Use Marcus MCP to create project from specification
import asyncio
from marcus_mcp import create_project

result = await create_project(
    project_name="Task Management API Demo",
    description=open("PROJECT_SPEC.md").read(),
    options={"complexity": "standard", "provider": "planka"}
)
```

This generates:
- Project board with tasks broken down by feature
- Task priorities and dependencies
- Estimated effort per task

### Phase 3: Deploy Multi-Agent Workers

Deploy 4 agents in parallel:
- **Agent 1**: Authentication & User Management
- **Agent 2**: Project Management endpoints
- **Agent 3**: Task Management endpoints
- **Agent 4**: Comments & Integration

Each agent:
1. Registers with Marcus
2. Requests assigned tasks
3. Implements endpoints per specification
4. Writes tests (targeting 80% coverage)
5. Reports progress at 25%, 50%, 75%, 100%
6. Commits code with descriptive messages

### Phase 4: Validation & Metrics

```bash
# Run validation suite
python validate_api.py

# Generates reports on:
# - API spec compliance (target: 100%)
# - Test coverage (target: 80%+)
# - Type safety (target: 0 mypy errors)
# - Integration success
```

## Quality Benchmarks

| Metric | Target | Measurement |
|--------|--------|-------------|
| API Spec Compliance | 100% | All 15 endpoints match OpenAPI spec |
| Test Coverage | ≥80% | pytest-cov |
| Type Safety | 0 errors | mypy |
| Response Times | <100ms | Performance tests |
| Integration | Pass | All agents' work integrates |

## Measurable Outcomes

**Speed**:
- Single agent: ~8-12 hours (sequential)
- Multi-agent: ~3-4 hours (parallel)
- **Speedup: 2.5-3x**

**Quality**:
- Test coverage: 80%+ (enforced)
- Type errors: 0 (enforced)
- API compliance: 100% (validated)
- All endpoints functional and integrated

## Running the Demo

### Autonomous Multi-Agent Mode (Recommended)

**Start the autonomous demo**:
```bash
python autonomous_agent_spawner.py
```

This will:
1. Spawn Process 1: Create the Marcus project
2. Spawn Processes 2-5: Four autonomous agents
3. Each agent registers 5 subagents (20 total)
4. Agents continuously request and complete tasks
5. All work tracked in MLflow experiments

**Monitor progress in real-time**:
```bash
# In a separate terminal
python monitor_agents.py
```

**View logs**:
```bash
tail -f logs/agent_foundation.log
tail -f logs/agent_auth.log
tail -f logs/agent_api.log
tail -f logs/agent_integration.log
```

### Manual Mode (Alternative)

1. **Create the project**:
   ```bash
   python quick_start.py --with-experiment
   ```

2. **Manually run agents**:
   Open multiple Claude Code terminals and follow Agent_prompt.md workflow

## Key Differentiators

1. **Automated Quality**: Not just fast, but enforces quality standards
2. **Measurable**: Clear metrics on speed and quality
3. **Realistic**: Builds actual working software, not toy examples
4. **Validated**: Automated compliance checking against specification
5. **Coordinated**: Demonstrates multi-agent collaboration

## Expected Results

After completion, you'll have:
- ✅ Working API with all 15 endpoints
- ✅ 80%+ test coverage
- ✅ Zero type errors
- ✅ 100% spec compliance
- ✅ Complete in 1/3 the time
- ✅ Production-ready code quality

## Next Steps

After this demo, you can extend it to show:
- Frontend integration (React/Vue agent)
- Deployment automation (DevOps agent)
- Documentation generation (Docs agent)
- Load testing (Performance agent)
